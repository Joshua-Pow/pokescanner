{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BestCNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Importing libraries / Data Loading and Splitting\n",
        "\n"
      ],
      "metadata": {
        "id": "-sqsjspqPsY1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torchvision import datasets, models, transforms\n",
        "from os import rename, listdir, path\n",
        "import math\n",
        "import shutil\n",
        "\n",
        "torch.manual_seed(1) # set the random seed\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True) #Must have folder inside drive called PokeScanner\n",
        "\n",
        "data_transform = transforms.Compose([\n",
        "    # you can add other transformations in this list\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "train_transform = transforms.Compose([\n",
        "    # you can add other transformations in this list\n",
        "    transforms.ToTensor(),\n",
        "    transforms.ColorJitter(0.1,0.1,0.1,0.1),\n",
        "    transforms.RandomPerspective(0.25)\n",
        "])"
      ],
      "metadata": {
        "id": "Ws6j7ioiPpHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = True\n",
        "# filePath = '/content/gdrive/MyDrive/PokeScanner' #Everyone has had this folder shared with them, Right click the folder and click \"Add shortcut to Drive\"\n",
        "\n",
        "# i = 0\n",
        "# j = 0\n",
        "# items = listdir(\"/content/gdrive/MyDrive/PokeScanner/PokemonDataCleaned\")\n",
        "# print(items)\n",
        "# for i in range (0, len(items)):\n",
        "#   if (os.path.isdir(\"/content/gdrive/MyDrive/PokeScanner/PokemonDataCleaned/\"+ items[i])):\n",
        "#     dirItems = listdir(\"/content/gdrive/MyDrive/PokeScanner/PokemonDataCleaned/\"+ items[i])\n",
        "#     trainingNumber = math.floor(len(dirItems) * 0.80) \n",
        "#     validationNumber = math.floor(len(dirItems) * 0.10)\n",
        "#     for j in range (0, len(dirItems)):\n",
        "#      if (os.path.isfile(\"/content/gdrive/MyDrive/PokeScanner/PokemonDataCleaned/\" + items[i] + \"/\" + dirItems[j])):\n",
        "#        if (j <= trainingNumber):\n",
        "#         shutil.copyfile(\"/content/gdrive/MyDrive/PokeScanner/PokemonDataCleaned/\" + items[i] + \"/\" + dirItems[j], \"/content/gdrive/MyDrive/PokeScanner/Training/\" + items[i] + \"/\" + dirItems[j])\n",
        "#        elif (j <= trainingNumber + validationNumber):\n",
        "#         shutil.copyfile(\"/content/gdrive/MyDrive/PokeScanner/PokemonDataCleaned/\" + items[i] + \"/\" + dirItems[j], \"/content/gdrive/MyDrive/PokeScanner/Validation/\" + items[i] + \"/\" + dirItems[j])\n",
        "#        else:\n",
        "#         shutil.copyfile(\"/content/gdrive/MyDrive/PokeScanner/PokemonDataCleaned/\" + items[i] + \"/\" + dirItems[j], \"/content/gdrive/MyDrive/PokeScanner/Testing/\" + items[i] + \"/\" + dirItems[j])\n",
        "\n",
        "# train_set = datasets.ImageFolder(\"/content/gdrive/MyDrive/PokeScanner/Training\")\n",
        "# validation_set = datasets.ImageFolder(\"/content/gdrive/MyDrive/PokeScanner/Validation\")\n",
        "# testing_set = datasets.ImageFolder(\"/content/gdrive/MyDrive/PokeScanner/Testing\")\n",
        "\n",
        "# print('Number of Training Images:', len(train_set))\n",
        "# print('Number of Validation Images:', len(validation_set))\n",
        "# print('Number of Testing Images:', len(testing_set))"
      ],
      "metadata": {
        "id": "4Kyp8HEg6iA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filePath = '/content/gdrive/MyDrive/PokeScanner' #Everyone has had this folder shared with them, Right click the folder and click \"Add shortcut to Drive\"\n",
        "train_set = datasets.ImageFolder(filePath+'/Training', transform=train_transform)\n",
        "validation_set = datasets.ImageFolder(filePath+'/Validation', transform=data_transform)\n",
        "testing_set = datasets.ImageFolder(filePath+'/Testing', transform=data_transform)"
      ],
      "metadata": {
        "id": "h6983FGSY0n9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2. Generating More Data"
      ],
      "metadata": {
        "id": "M6yrwikbUL7a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Model Building and Sanity Checking\n",
        "\n",
        "Build a convolutional neural network model that takes the (224x224 RGB) image as input, and predicts the pokemon"
      ],
      "metadata": {
        "id": "BTGSzk07P2m_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PokemonClassifier(nn.Module):\n",
        "      def __init__(self, kernel_size):\n",
        "          super(PokemonClassifier, self).__init__()\n",
        "          self.conv1 = nn.Conv2d(3, 5, kernel_size) #in_channels, out_chanels, kernel_size\n",
        "          self.dropout = nn.Dropout(0.1)#dropout layer\n",
        "          self.pool = nn.MaxPool2d(2, 2) #kernel_size, stride \n",
        "          self.conv2 = nn.Conv2d(5, 10, kernel_size) #in_channels, out_chanels, kernel_size \n",
        "          self.conv3 = nn.Conv2d(7, 10, kernel_size) #in_channels, out_chanels, kernel_size\n",
        "          shape = math.floor((251 - kernel_size - 2) / 2) + 1\n",
        "          shape2 = math.floor((shape - kernel_size - 1) / 2) + 1\n",
        "          shape3 = math.floor((shape2 - kernel_size - 1) / 2) + 1\n",
        "          self.fc1 = nn.Linear(10 * shape2 * shape2, 200) #needs input-dependent value\n",
        "          self.fc2 = nn.Linear(200, 150)\n",
        "\n",
        "      def forward(self, x):\n",
        "          x = self.pool(F.relu(self.conv1(x)))\n",
        "          #print(x.shape)\n",
        "          x = self.pool(F.relu(self.conv2(x)))\n",
        "          #print(x.shape)\n",
        "          x = self.pool(F.relu(self.conv3(x)))\n",
        "          #print(x.shape)\n",
        "          x = x.view(-1, 10 * x.shape[2] * x.shape[2])\n",
        "          #print(x.shape)\n",
        "          x = self.dropout(x)\n",
        "          x = F.relu(self.fc1(x))\n",
        "          #print(x.shape)\n",
        "          x = self.dropout(x)\n",
        "          x = self.fc2(x)\n",
        "          #print(x.shape)\n",
        "          return x"
      ],
      "metadata": {
        "id": "j8v_N94OP1LD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#From tut3a\n",
        "def get_accuracy(model, accuracy_data, batch_size):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for imgs, labels in torch.utils.data.DataLoader(accuracy_data, batch_size=batch_size):\n",
        "        \n",
        "        \n",
        "        #############################################\n",
        "        #To Enable GPU Usage\n",
        "        if use_cuda and torch.cuda.is_available():\n",
        "          imgs = imgs.cuda()\n",
        "          labels = labels.cuda()\n",
        "        else:\n",
        "          print(\"get_accuracy: no cuda\")\n",
        "        #############################################\n",
        "        \n",
        "        \n",
        "        output = model(imgs)\n",
        "        \n",
        "        #select index with maximum prediction score\n",
        "        pred = output.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
        "        total += imgs.shape[0]\n",
        "    return correct / total"
      ],
      "metadata": {
        "id": "W4tMq-H09Tpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#From tut3a\n",
        "def train(model, data, size, num_epochs, learning_rate):\n",
        "    train_loader = torch.utils.data.DataLoader(data, batch_size=size, shuffle=True)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
        "\n",
        "    iters, losses, train_acc, val_acc, epochList = [], [], [], [], []\n",
        "\n",
        "    # training\n",
        "    n = 0 # the number of iterations\n",
        "    for epoch in range(num_epochs):\n",
        "        for imgs, labels in iter(train_loader):\n",
        "\n",
        "            #############################################\n",
        "            #To Enable GPU Usage\n",
        "            if use_cuda and torch.cuda.is_available():\n",
        "              imgs = imgs.cuda()\n",
        "              labels = labels.cuda()\n",
        "            else:\n",
        "              print(\"Train: no cuda\")\n",
        "            #############################################\n",
        "            # print(\"img: \", imgs.shape)\n",
        "            # print(\"label: \", labels)\n",
        "            out = model(imgs)             # forward pass\n",
        "            \n",
        "            loss = criterion(out, labels) # compute the total loss\n",
        "            loss.backward()               # backward pass (compute parameter updates)\n",
        "            optimizer.step()              # make the updates for each parameter\n",
        "            optimizer.zero_grad()         # a clean up step for PyTorch\n",
        "\n",
        "            # save the current training information\n",
        "            iters.append(n)\n",
        "            losses.append(float(loss)/size)             # compute *average* loss\n",
        "            n += 1\n",
        "        train_acc.append(get_accuracy(model, accuracy_data=train_set, batch_size=size)) # compute training accuracy \n",
        "        val_acc.append(get_accuracy(model, accuracy_data=validation_set, batch_size=size))  # compute validation accuracy\n",
        "        epochList.append(epoch)\n",
        "        print(\"Epoch {}: Training Accuracy: {} Validation Accuracy: {}\".format(epoch, train_acc[epoch], val_acc[epoch]))\n",
        "    \n",
        "    # plotting\n",
        "    plt.title(\"Training Curve\")\n",
        "    plt.plot(iters, losses, label=\"Train\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.title(\"Training Curve\")\n",
        "    plt.plot(epochList, train_acc, label=\"Train\")\n",
        "    plt.plot(epochList, val_acc, label=\"Validation\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Training Accuracy\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Final Training Accuracy: {}\".format(train_acc[-1]))\n",
        "    print(\"Final Validation Accuracy: {}\".format(val_acc[-1]))\n",
        "  \n"
      ],
      "metadata": {
        "id": "sorZBi_L9SdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = PokemonClassifier(3)\n",
        "\n",
        "#proper model\n",
        "#############################################\n",
        "#To Enable GPU Usage\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "  print(\"cuda\")\n",
        "#############################################\n",
        "\n",
        "#Basic first attempt at training\n",
        "train(model, train_set, size=64, num_epochs=50, learning_rate=0.006)"
      ],
      "metadata": {
        "id": "9WDrZfbl9Yai"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}